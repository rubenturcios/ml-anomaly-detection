{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from haversine import haversine, Unit\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('./env.env')\n",
    "\n",
    "driver_name = 'postgresql+psycopg2'\n",
    "url = URL.create(\n",
    "    drivername=driver_name,\n",
    "    username=os.environ['USERNAME'],\n",
    "    password=os.environ['PASSWORD'],\n",
    "    host=os.environ['HOST'],\n",
    "    port=os.environ['PORT'],\n",
    "    database=os.environ['DB']\n",
    ")\n",
    "engine = create_engine(url)\n",
    "db_connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select h.id as event_id, h.facility_uuid, v.uid as visit_facility_uuid, h.\"timestamp\", h.latitude as event_latitude,\n",
    "h.longitude as event_longitude, h.\"event\", h.altitude,\n",
    "v.project_id, v.latitude as facility_latitude, v.longitude as facility_longitude, v.revision,\n",
    "v.inspection_document, v.person_of_interest_id\n",
    "from custom h join visit v on v.uid = h.facility_uuid join usr u on v.person_of_interest_id = u.id where h.\"event\" = 9 or h.\"event\" = 13 order by h.\"timestamp\" desc\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_facility_uuid = [y for _, y in df.groupby('facility_uuid')]\n",
    "print(len(df_per_facility_uuid))\n",
    "\n",
    "\n",
    "def get_datetime(date_time: str) -> datetime:\n",
    "    index = date_time.rfind(' ')\n",
    "    date_time = date_time[:index] + date_time[index + 1:]\n",
    "    return datetime.fromisoformat(date_time)\n",
    "\n",
    "\n",
    "def get_elapsed_time(t1: str, t2: str) -> int:\n",
    "    datetime_1 = get_datetime(t1)\n",
    "    datetime_2 = get_datetime(t2)\n",
    "    return (datetime_1 - datetime_2).seconds\n",
    "\n",
    "\n",
    "start_time = str()\n",
    "\n",
    "for df_ in df_per_facility_uuid:\n",
    "    number_of_save_and_exit_counts = []\n",
    "    elapsed_times = []\n",
    "    distances = []\n",
    "    number_of_save_and_exit_count = 0\n",
    "\n",
    "    for i in df_.sort_values(by='timestamp').index:\n",
    "        if df_['event'][i] == 13:\n",
    "            start_time = df_['timestamp'][i]\n",
    "            elapsed_times.append(0)\n",
    "            distances.append(0)\n",
    "            number_of_save_and_exit_counts.append(0)\n",
    "        elif df_['event'][i] == 9:\n",
    "            number_of_save_and_exit_count += 1\n",
    "            number_of_save_and_exit_counts.append(number_of_save_and_exit_count)\n",
    "\n",
    "            end_time = df_['timestamp'][i]\n",
    "            elapsed_times.append((end_time - start_time).seconds)\n",
    "\n",
    "            event_lat = df_['event_latitude'][i]\n",
    "            event_long = df_['event_longitude'][i]\n",
    "            event = (event_lat, event_long)\n",
    "\n",
    "            fac_lat = df_['facility_latitude'][i]\n",
    "            fac_long = df_['facility_longitude'][i]\n",
    "            facility = (fac_lat, fac_long)\n",
    "\n",
    "            distances.append(haversine(event, facility, Unit.METERS))\n",
    "\n",
    "    elapsed_times.reverse()\n",
    "    distances.reverse()\n",
    "    number_of_save_and_exit_counts.reverse()\n",
    "    df_['elapsed_time'] = elapsed_times\n",
    "    df_['distance'] = distances\n",
    "    df_['save_and_exit_count'] = number_of_save_and_exit_counts\n",
    "    #print(df_[['event', 'timestamp', 'elapsed_time', 'distance']])\n",
    "\n",
    "print(df_per_facility_uuid[0][['event', 'timestamp', 'elapsed_time', 'distance', 'save_and_exit_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_per_facility_uuid, ignore_index=True)\n",
    "#print(df[df['event'] == 9][['event', 'timestamp', 'elapsed_time', 'distance']])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_joined = pd.concat((df, df_inpsection_documents), axis=1)\n",
    "#print(len(final_joined))\n",
    "\n",
    "#df_save_and_exit_only = final_joined[final_joined['event'] == 9]\n",
    "df_save_and_exit_only = df[df['event'] == 9]\n",
    "data = df_save_and_exit_only[['elapsed_time', 'distance', 'revision', 'save_and_exit_count']]\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr = data.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(corr, cmap=\"Blues\", annot=True, square=False, ax=ax)\n",
    "plt.title('Pearson Correlation of Features')\n",
    "plt.yticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from pyod.models.iforest import IForest\n",
    "\n",
    "\n",
    "# Create a model with 10000 trees\n",
    "iforest = IForest(n_estimators=1000)\n",
    "iforest.fit(data)  # This will take a minute\n",
    "\n",
    "# Extract the labels\n",
    "labels = iforest.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outlier_free = df_save_and_exit_only[labels == 0]\n",
    "X_outlier = df_save_and_exit_only[labels == 1]\n",
    "\n",
    "print(len(X_outlier_free))\n",
    "print(len(data))\n",
    "print(len(df_save_and_exit_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_mapped = pd.Series({column: importance for column, importance in zip(data.columns, iforest.feature_importances_)})\n",
    "print(feature_importance_mapped.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save_and_exit_only['score'] = iforest.decision_scores_\n",
    "print('Threshold Value: %s' % iforest.threshold_)\n",
    "sns.displot(df_save_and_exit_only, x='score', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_median = df_save_and_exit_only['score'].median()\n",
    "percentile_90 = df_save_and_exit_only['score'].quantile(0.9)\n",
    "percentile_99 = df_save_and_exit_only['score'].quantile(.99)\n",
    "\n",
    "print('Median Score: %s' % scores_median)\n",
    "print('90th Percentile Score: %s' % percentile_90)\n",
    "\n",
    "lower_fifty = df_save_and_exit_only[df_save_and_exit_only.score<scores_median].sort_values(by='score', ascending=False)\n",
    "upper_fifty = df_save_and_exit_only[df_save_and_exit_only.score>=scores_median].sort_values(by='score', ascending=False)\n",
    "upper_90 = df_save_and_exit_only[df_save_and_exit_only.score>=percentile_90].sort_values(by='score', ascending=False)\n",
    "upper_99 = df_save_and_exit_only[df_save_and_exit_only.score>=percentile_99].sort_values(by='score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_50th_lower = lower_fifty.iloc[0]\n",
    "example_50th_upper = upper_fifty.iloc[-1]\n",
    "example_90th_highest = upper_90.iloc[0]\n",
    "example_90th_lowest = upper_90.iloc[-1]\n",
    "\n",
    "print('Example Lower 50th -- Facility UUID: %s, Score: %s' % (example_50th_lower['facility_uuid'], example_50th_lower['score']))\n",
    "print('Example Upper 50th -- Facility UUID: %s, Score: %s' % (example_50th_upper['facility_uuid'], example_50th_upper['score']))\n",
    "print('Example Lowest 90th -- Facility UUID: %s, Score: %s' % (example_90th_lowest['facility_uuid'], example_90th_lowest['score']))\n",
    "print('Example Highest 90th -- Facility UUID: %s, Score: %s' % (example_90th_highest['facility_uuid'], example_90th_highest['score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df[df.scores > 0.1].head())\n",
    "print(data.elapsed_time.median())\n",
    "print(data.distance.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.displot(data, x='elapsed_time')\n",
    "elapsed_times_boxplot = sns.boxplot(data, x='elapsed_time')\n",
    "data[data['elapsed_time'] > 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data, x='distance')\n",
    "data[data['distance'] > 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(upper_99))\n",
    "upper_99.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "clf = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "y_pred = clf.fit_predict(data)\n",
    "#X_scores = clf.negative_outlier_factor_\n",
    "X_scores = iforest.decision_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerPathCollection\n",
    "\n",
    " \n",
    "# explicit function to normalize array\n",
    "def normalize(arr, t_min, t_max):\n",
    "    norm_arr = []\n",
    "    diff = t_max - t_min\n",
    "    diff_arr = max(arr) - min(arr)    \n",
    "    for i in arr:\n",
    "        temp = (((i - min(arr))*diff)/diff_arr) + t_min\n",
    "        norm_arr.append(temp)\n",
    "    return np.array(norm_arr)\n",
    "\n",
    "\n",
    "def update_legend_marker_size(handle, orig):\n",
    "    \"Customize size of the legend marker\"\n",
    "    handle.update_from(orig)\n",
    "    handle.set_sizes([20])\n",
    "\n",
    "\n",
    "plt.scatter(data['elapsed_time'].values, data['distance'].values, color=\"k\", s=3.0, label=\"Data points\")\n",
    "# plot circles with radius proportional to the outlier scores\n",
    "normalized = normalize(X_scores, 0, 1)\n",
    "radius = (normalized.max() - normalized) / (normalized.max() - normalized.min())\n",
    "scatter = plt.scatter(\n",
    "    data['elapsed_time'].values,\n",
    "    data['distance'].values,\n",
    "    s=100,\n",
    "    c=X_scores,\n",
    "    edgecolors=\"r\",\n",
    "    facecolors=\"none\",\n",
    "    label=\"Outlier scores\",\n",
    ")\n",
    "plt.axis(\"tight\")\n",
    "#plt.xlim((-5, 5))\n",
    "#plt.ylim((-5, 5))\n",
    "plt.xlabel(\"Elapsed Time (s)\")\n",
    "plt.ylabel('Distance (m)')\n",
    "plt.legend(\n",
    "    handler_map={scatter: HandlerPathCollection(update_func=update_legend_marker_size)}\n",
    ")\n",
    "plt.title(\"Local Outlier Factor (LOF)\")\n",
    "plt.colorbar().set_label('Outlier Score', rotation=270)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_grouped_by_person_of_interest = [y for _, y in df_save_and_exit_only.groupby('person_of_interest_id')]\n",
    "dfs_grouped_by_person_of_interest[0].sort_values(by='timestamp', ascending=False, inplace=True)\n",
    "g = dfs_grouped_by_person_of_interest[0].rolling(window='7d', closed='both', on='timestamp').distance.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_grouped_by_person_of_interest[0]['mean_distance'] = g.values\n",
    "dfs_grouped_by_person_of_interest[0][['timestamp', 'distance', 'mean_distance']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_grouped_by_person_of_interest = [y for _, y in df_save_and_exit_only.groupby('person_of_interest_id')]\n",
    "print(len(dfs_grouped_by_person_of_interest))\n",
    "\n",
    "for df_ in dfs_grouped_by_person_of_interest:\n",
    "    df_.sort_values(by='timestamp', ascending=False, inplace=True)\n",
    "    window = df_.rolling(window='7d', on='timestamp', closed='both')\n",
    "    mean_distance = window.distance.mean()\n",
    "    mean_elapsed_time = window.elapsed_time.mean()\n",
    "    mean_revision = window.revision.mean()\n",
    "    mean_save_and_exit_count = window.save_and_exit_count.mean()\n",
    "\n",
    "    df_['mean_distance'] = mean_distance.values\n",
    "    df_['mean_elapsed_time'] = mean_elapsed_time.values\n",
    "    df_['mean_revision'] = mean_revision.values\n",
    "    df_['mean_save_and_exit_count'] = mean_save_and_exit_count.values\n",
    "\n",
    "df_with_means = pd.concat(dfs_grouped_by_person_of_interest)\n",
    "df_with_means.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = df_with_means[['mean_distance', 'mean_elapsed_time', 'mean_revision', 'mean_save_and_exit_count']]\n",
    "corr = data.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(corr, cmap=\"Blues\", annot=True, square=False, ax=ax)\n",
    "plt.title('Pearson Correlation of Features')\n",
    "plt.yticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_person_of_interests = IForest()\n",
    "clf_person_of_interests.fit(data)\n",
    "\n",
    "# Extract the labels\n",
    "person_of_interests_labels = clf_person_of_interests.labels_\n",
    "feature_importance_mapped = pd.Series({column: importance for column, importance in zip(data.columns, clf_person_of_interests.feature_importances_)})\n",
    "print(feature_importance_mapped.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_means['person_of_interest_score'] = clf_person_of_interests.decision_scores_\n",
    "\n",
    "print('Threshold Value: %s' % clf_person_of_interests.threshold_)\n",
    "sns.displot(df_with_means, x='person_of_interest_score', kde=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
